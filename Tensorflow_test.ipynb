{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5898efce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs detected:\", gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7fb19b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow\n",
      "  Using cached pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Using cached pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Installing collected packages: pillow\n",
      "Successfully installed pillow-11.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.11/site-packages (1.26.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: keras in ./venv/lib/python3.11/site-packages (3.11.3)\n",
      "Requirement already satisfied: absl-py in ./venv/lib/python3.11/site-packages (from keras) (2.3.1)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.11/site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in ./venv/lib/python3.11/site-packages (from keras) (14.1.0)\n",
      "Requirement already satisfied: namex in ./venv/lib/python3.11/site-packages (from keras) (0.1.0)\n",
      "Requirement already satisfied: h5py in ./venv/lib/python3.11/site-packages (from keras) (3.14.0)\n",
      "Requirement already satisfied: optree in ./venv/lib/python3.11/site-packages (from keras) (0.17.0)\n",
      "Requirement already satisfied: ml-dtypes in ./venv/lib/python3.11/site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from keras) (25.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in ./venv/lib/python3.11/site-packages (from optree->keras) (4.15.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.11/site-packages (from rich->keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.11/site-packages (from rich->keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.60.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (111 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in ./venv/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.11/site-packages (from matplotlib) (11.3.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.4-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Using cached matplotlib-3.10.6-cp311-cp311-macosx_11_0_arm64.whl (8.1 MB)\n",
      "Using cached contourpy-1.3.3-cp311-cp311-macosx_11_0_arm64.whl (270 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.60.0-cp311-cp311-macosx_10_9_universal2.whl (2.8 MB)\n",
      "Using cached kiwisolver-1.4.9-cp311-cp311-macosx_11_0_arm64.whl (65 kB)\n",
      "Using cached pyparsing-3.2.4-py3-none-any.whl (113 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [matplotlib]6\u001b[0m [matplotlib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.0 kiwisolver-1.4.9 matplotlib-3.10.6 pyparsing-3.2.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting lxml\n",
      "  Using cached lxml-6.0.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.8 kB)\n",
      "Using cached lxml-6.0.1-cp311-cp311-macosx_10_9_universal2.whl (8.4 MB)\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-6.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting keras_cv\n",
      "  Using cached keras_cv-0.9.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from keras_cv) (25.0)\n",
      "Requirement already satisfied: absl-py in ./venv/lib/python3.11/site-packages (from keras_cv) (2.3.1)\n",
      "Collecting regex (from keras_cv)\n",
      "  Downloading regex-2025.9.18-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tensorflow-datasets (from keras_cv)\n",
      "  Using cached tensorflow_datasets-4.9.9-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting keras-core (from keras_cv)\n",
      "  Using cached keras_core-0.1.7-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting kagglehub (from keras_cv)\n",
      "  Using cached kagglehub-0.3.13-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting pyyaml (from kagglehub->keras_cv)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.11/site-packages (from kagglehub->keras_cv) (2.32.5)\n",
      "Collecting tqdm (from kagglehub->keras_cv)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.11/site-packages (from keras-core->keras_cv) (1.26.4)\n",
      "Requirement already satisfied: rich in ./venv/lib/python3.11/site-packages (from keras-core->keras_cv) (14.1.0)\n",
      "Requirement already satisfied: namex in ./venv/lib/python3.11/site-packages (from keras-core->keras_cv) (0.1.0)\n",
      "Requirement already satisfied: h5py in ./venv/lib/python3.11/site-packages (from keras-core->keras_cv) (3.14.0)\n",
      "Collecting dm-tree (from keras-core->keras_cv)\n",
      "  Using cached dm_tree-0.1.9-cp311-cp311-macosx_10_9_universal2.whl.metadata (2.4 kB)\n",
      "Collecting attrs>=18.2.0 (from dm-tree->keras-core->keras_cv)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in ./venv/lib/python3.11/site-packages (from dm-tree->keras-core->keras_cv) (1.17.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests->kagglehub->keras_cv) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests->kagglehub->keras_cv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests->kagglehub->keras_cv) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests->kagglehub->keras_cv) (2025.8.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.11/site-packages (from rich->keras-core->keras_cv) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.11/site-packages (from rich->keras-core->keras_cv) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras_cv) (0.1.2)\n",
      "Collecting etils>=1.9.1 (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv)\n",
      "  Using cached etils-1.13.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting immutabledict (from tensorflow-datasets->keras_cv)\n",
      "  Using cached immutabledict-4.2.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting promise (from tensorflow-datasets->keras_cv)\n",
      "  Using cached promise-2.3-py3-none-any.whl\n",
      "Requirement already satisfied: protobuf>=3.20 in ./venv/lib/python3.11/site-packages (from tensorflow-datasets->keras_cv) (4.25.8)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.11/site-packages (from tensorflow-datasets->keras_cv) (7.1.0)\n",
      "Collecting pyarrow (from tensorflow-datasets->keras_cv)\n",
      "  Using cached pyarrow-21.0.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting simple_parsing (from tensorflow-datasets->keras_cv)\n",
      "  Using cached simple_parsing-0.1.7-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets->keras_cv)\n",
      "  Using cached tensorflow_metadata-1.17.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: termcolor in ./venv/lib/python3.11/site-packages (from tensorflow-datasets->keras_cv) (3.1.0)\n",
      "Collecting toml (from tensorflow-datasets->keras_cv)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting einops (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fsspec (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting importlib_resources (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing_extensions in ./venv/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (4.15.0)\n",
      "Collecting zipp (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv)\n",
      "  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.11/site-packages (from promise->tensorflow-datasets->keras_cv) (1.17.0)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple_parsing->tensorflow-datasets->keras_cv)\n",
      "  Using cached docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow-datasets->keras_cv)\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Using cached keras_cv-0.9.0-py3-none-any.whl (650 kB)\n",
      "Using cached kagglehub-0.3.13-py3-none-any.whl (68 kB)\n",
      "Using cached keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
      "Using cached dm_tree-0.1.9-cp311-cp311-macosx_10_9_universal2.whl (173 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
      "Downloading regex-2025.9.18-cp311-cp311-macosx_11_0_arm64.whl (286 kB)\n",
      "Using cached tensorflow_datasets-4.9.9-py3-none-any.whl (5.3 MB)\n",
      "Using cached etils-1.13.0-py3-none-any.whl (170 kB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
      "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached pyarrow-21.0.0-cp311-cp311-macosx_12_0_arm64.whl (31.2 MB)\n",
      "Using cached simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
      "Using cached docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Using cached tensorflow_metadata-1.17.2-py3-none-any.whl (31 kB)\n",
      "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: zipp, tqdm, toml, regex, pyyaml, pyarrow, promise, importlib_resources, immutabledict, googleapis-common-protos, fsspec, etils, einops, docstring-parser, attrs, tensorflow-metadata, simple_parsing, kagglehub, dm-tree, keras-core, tensorflow-datasets, keras_cv\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/22\u001b[0m [keras_cv]/22\u001b[0m [keras_cv]w-datasets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed attrs-25.3.0 dm-tree-0.1.9 docstring-parser-0.17.0 einops-0.8.1 etils-1.13.0 fsspec-2025.9.0 googleapis-common-protos-1.70.0 immutabledict-4.2.1 importlib_resources-6.5.2 kagglehub-0.3.13 keras-core-0.1.7 keras_cv-0.9.0 promise-2.3 pyarrow-21.0.0 pyyaml-6.0.2 regex-2025.9.18 simple_parsing-0.1.7 tensorflow-datasets-4.9.9 tensorflow-metadata-1.17.2 toml-0.10.2 tqdm-4.67.1 zipp-3.23.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "!pip install pillow \n",
    "!pip install numpy \n",
    "!pip install keras \n",
    "!pip install matplotlib \n",
    "!pip install lxml \n",
    "!pip install keras_cv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33c941d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calebkjoseph/Test/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#if you are using tensorflow metal you cannot use tensorflow hub\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "\n",
    "import keras_cv\n",
    "from keras_cv import bounding_box\n",
    "from keras_cv import visualization\n",
    "import matplotlib\n",
    "\n",
    "import os \n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015c7136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids = [\n",
    "    \"ThumbsDown\",\n",
    "    \"ThumbsUp\"\n",
    "]\n",
    "class_mapping = dict(zip(range(len(class_ids)), class_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4833143",
   "metadata": {},
   "source": [
    "Function below is able to process the images and annotations and put them in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_array(path_to_image): \n",
    "    \"\"\"Function loads in a image and outputs them as a numpy array\"\"\"\n",
    "    loaded_image = Image.open(os.path.join(path_to_image)) #opens the image and saves it in loarded image\n",
    "    Array = np.array(loaded_image)/255 # makes a numpy array of the loaded image and standardizes it's values between 0-1\n",
    "    return Array\n",
    "\n",
    "def read_annotation_file(xml_file):\n",
    "    \"\"\"\n",
    "    Reads a Pascal VOC-style XML file and returns bounding boxes as a NumPy array.\n",
    "    \"\"\"\n",
    "    tree = etree.parse(xml_file) #parses xml file\n",
    "    root = tree.getroot() #gets the top indice of the xml file\n",
    "    \n",
    "    boxes = []\n",
    "    \n",
    "    for obj in root.findall(\"object\"): #gets the objects in the object subclass\n",
    "        name = obj.find(\"name\").text #gets what's the name of this object\n",
    "        bndbox = obj.find(\"bndbox\") #goes into the boundbox subclass\n",
    "        xmin = int(bndbox.find(\"xmin\").text) #grabs the differenct xmins\n",
    "        ymin = int(bndbox.find(\"ymin\").text)\n",
    "        xmax = int(bndbox.find(\"xmax\").text)\n",
    "        ymax = int(bndbox.find(\"ymax\").text)\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "    \n",
    "    return np.array(boxes), name\n",
    "\n",
    "def get_x_and_y(path_to_images, path_to_annotations):\n",
    "    \"\"\"outputs an array of images and an array of annotations given a path to both files\"\"\"\n",
    "    images_array = []\n",
    "    bboxes_array = []\n",
    "    names_array = []\n",
    "    for image in os.listdir(path_to_images): \n",
    "        annotation_file_name = os.path.splitext(image)[0] + '.xml' #gets the annotation file name \n",
    "        annotation_file_path = os.path.join(path_to_annotations,annotation_file_name) #gets the path \n",
    "        BBoxes, Label_Name = read_annotation_file(annotation_file_path) # reads the files\n",
    "        Image = image_to_array(os.path.join(path_to_images,image)) \n",
    "        images_array.append(Image)\n",
    "        bboxes_array.append(BBoxes)\n",
    "        names_array.append(Label_Name)\n",
    "    images_array = np.array(images_array)\n",
    "    bboxes_array = np.array(bboxes_array) \n",
    "    names_array = np.array(names_array)\n",
    "    names_array = np.reshape(names_array,(198,1))\n",
    "    return images_array, bboxes_array, names_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79724c2e",
   "metadata": {},
   "source": [
    "Next extract the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2fdda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198, 640, 640, 3) (198, 1, 4) (198, 1)\n",
      "tf.Tensor(\n",
      "[[2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]], shape=(198, 1), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 21:05:09.216162: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4\n",
      "2025-09-18 21:05:09.216199: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-09-18 21:05:09.216206: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-09-18 21:05:09.216236: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-09-18 21:05:09.216246: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "images, bboxes, classes = get_x_and_y(\"Workspace/GoodOrBad/Images/ProcessedImages/\",\"Workspace/GoodOrBad/Annotations/\")\n",
    "\n",
    "lookup = tf.keras.layers.StringLookup(output_mode=\"int\") #to change the classes from strings to integers\n",
    "lookup.adapt(classes) #checks how many classes are in classes\n",
    "classes = lookup(classes) #encodes it\n",
    "print(classes)\n",
    "images = tf.constant(images) #creates tensors for each dataset\n",
    "bboxes = tf.constant(bboxes)\n",
    "classes = tf.constant(classes)\n",
    "\n",
    "data = tf.data.Dataset.from_tensor_slices((images, classes, bboxes)) #turns them into a dataset\n",
    "\n",
    "num_val = int(len(images) * .1) #creates a number of images to partition\n",
    "val_data = data.take(num_val)\n",
    "train_data = data.skip(num_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510cf10e",
   "metadata": {},
   "source": [
    "Next we need to edit the data that way it can be passed into the yolo model\n",
    "\n",
    "yolo expects datasets to be in this format:\n",
    "\n",
    "bounding_boxes = {\n",
    "    # num_boxes may be a Ragged dimension\n",
    "    'boxes': Tensor(shape=[batch, num_boxes, 4]),\n",
    "    'classes': Tensor(shape=[batch, num_boxes])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ad8b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_dataset(dataset, num_images=4):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i, (image, target) in enumerate(dataset.take(num_images)):\n",
    "        img = image.numpy().astype(\"float32\")\n",
    "\n",
    "        # target is a dict with \"boxes\" and \"classes\"\n",
    "        boxes = target[\"boxes\"].numpy()\n",
    "        labels = target[\"classes\"].numpy()\n",
    "\n",
    "        ax = plt.subplot(2, 2, i+1)\n",
    "        ax.imshow(img)\n",
    "\n",
    "        for j, (xmin, ymin, xmax, ymax) in enumerate(boxes):\n",
    "            rect = patches.Rectangle(\n",
    "                (xmin, ymin),\n",
    "                xmax - xmin,\n",
    "                ymax - ymin,\n",
    "                linewidth=2,\n",
    "                edgecolor=\"red\",\n",
    "                facecolor=\"none\"\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            ax.text(xmin, ymin - 5, str(int(labels[j])),\n",
    "                    color=\"yellow\", fontsize=10, weight=\"bold\")\n",
    "\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b861497",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(images, classes, bbox):\n",
    "    # Read Image\n",
    "    \n",
    "    bounding_boxes = {\n",
    "        \"classes\": tf.cast(classes, dtype=tf.float32),\n",
    "        \"boxes\": tf.cast(bbox, dtype=tf.float32),\n",
    "    }\n",
    "    return {\"images\": tf.cast(images, tf.float32), \"bounding_boxes\": bounding_boxes}\n",
    "\n",
    "augmenter = keras.Sequential(\n",
    "    layers=[\n",
    "        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xyxy\"),\n",
    "        keras_cv.layers.RandomShear(\n",
    "            x_factor=0.2, y_factor=0.2, bounding_box_format=\"xyxy\"\n",
    "        ),\n",
    "        keras_cv.layers.JitteredResize(\n",
    "            target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format=\"xyxy\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_ds = train_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(16)#16 is the batch size times 4 that way it doesn't batch similar images\n",
    "\n",
    "train_ds = train_ds.padded_batch(4, drop_remainder=True)\n",
    "#train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "#same for the valuation dataset\n",
    "val_ds = val_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.shuffle(16)\n",
    "val_ds = val_ds.padded_batch(4, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "141ea3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the dictionary in the datasets tuples\n",
    "\n",
    "def dict_to_tuple(inputs):\n",
    "    return inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#visualize_dataset(train_ds)\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE) #this line makes it so that it loads the next batch as the current is processing\n",
    "\n",
    "\n",
    "val_ds = val_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a312a2",
   "metadata": {},
   "source": [
    "Next we define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9454b96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = keras_cv.models.YOLOV8Backbone.from_preset(\n",
    "    \"yolo_v8_xs_backbone_coco\"  # We will use yolov8 extra small backbone with coco weights but you have other options\n",
    ")\n",
    "#below is how you define the model\n",
    "yolo = keras_cv.models.YOLOV8Detector(\n",
    "    num_classes= 2,\n",
    "    bounding_box_format=\"xyxy\",\n",
    "    backbone=backbone,\n",
    "    fpn_depth=1,\n",
    ")\n",
    "\n",
    "#defining the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    global_clipnorm=10.0,\n",
    ")\n",
    "\n",
    "yolo.compile(\n",
    "    optimizer=optimizer, classification_loss=\"binary_crossentropy\", box_loss=\"ciou\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e414bb",
   "metadata": {},
   "source": [
    "Next we create our callback. Imma be honest idk how this next few lines of code works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9839fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateCOCOMetricsCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, data, save_path):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.metrics = keras_cv.metrics.BoxCOCOMetrics(\n",
    "            bounding_box_format=\"xyxy\",\n",
    "            evaluate_freq=1e9,\n",
    "        )\n",
    "\n",
    "        self.save_path = save_path\n",
    "        self.best_map = -1.0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self.metrics.reset_state()\n",
    "        for batch in self.data:\n",
    "            images, y_true = batch[0], batch[1]\n",
    "            y_pred = self.model.predict(images, verbose=0)\n",
    "            self.metrics.update_state(y_true, y_pred)\n",
    "\n",
    "        metrics = self.metrics.result(force=True)\n",
    "        logs.update(metrics)\n",
    "\n",
    "        current_map = metrics[\"MaP\"]\n",
    "        if current_map > self.best_map:\n",
    "            self.best_map = current_map\n",
    "            self.model.save(self.save_path)  # Save the model when mAP improves\n",
    "\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fcd8fe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 1s/step - box_loss: 1.6263 - class_loss: 0.6693 - loss: 2.2956 - val_box_loss: 2.6999 - val_class_loss: 1.9908 - val_loss: 4.6907\n",
      "Epoch 2/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 975ms/step - box_loss: 1.4249 - class_loss: 0.7063 - loss: 2.1312 - val_box_loss: 2.6898 - val_class_loss: 2.1300 - val_loss: 4.8198\n",
      "Epoch 3/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 722ms/step - box_loss: 1.4225 - class_loss: 0.6217 - loss: 2.0442 - val_box_loss: 1.8611 - val_class_loss: 2.8569 - val_loss: 4.7179\n",
      "Epoch 4/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 870ms/step - box_loss: 1.3356 - class_loss: 0.6040 - loss: 1.9396 - val_box_loss: 3.3043 - val_class_loss: 2.9250 - val_loss: 6.2292\n",
      "Epoch 5/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 1s/step - box_loss: 1.3922 - class_loss: 0.5222 - loss: 1.9144 - val_box_loss: 3.5123 - val_class_loss: 2.5316 - val_loss: 6.0439\n",
      "Epoch 6/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 512ms/step - box_loss: 1.3212 - class_loss: 0.5556 - loss: 1.8767 - val_box_loss: 3.0318 - val_class_loss: 2.7714 - val_loss: 5.8032\n",
      "Epoch 7/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 519ms/step - box_loss: 1.4282 - class_loss: 0.5271 - loss: 1.9554 - val_box_loss: 3.3432 - val_class_loss: 2.5313 - val_loss: 5.8745\n",
      "Epoch 8/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 527ms/step - box_loss: 1.2671 - class_loss: 0.4821 - loss: 1.7492 - val_box_loss: 3.5374 - val_class_loss: 2.4017 - val_loss: 5.9390\n",
      "Epoch 9/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 527ms/step - box_loss: 1.3047 - class_loss: 0.5091 - loss: 1.8138 - val_box_loss: 3.9618 - val_class_loss: 2.5299 - val_loss: 6.4917\n",
      "Epoch 10/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 523ms/step - box_loss: 1.2091 - class_loss: 0.4555 - loss: 1.6646 - val_box_loss: 2.4641 - val_class_loss: 2.8242 - val_loss: 5.2884\n",
      "Epoch 11/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 564ms/step - box_loss: 1.1355 - class_loss: 0.4239 - loss: 1.5594 - val_box_loss: 2.8677 - val_class_loss: 2.9800 - val_loss: 5.8477\n",
      "Epoch 12/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 632ms/step - box_loss: 1.1890 - class_loss: 0.4799 - loss: 1.6689 - val_box_loss: 2.9604 - val_class_loss: 3.2227 - val_loss: 6.1830\n",
      "Epoch 13/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 634ms/step - box_loss: 1.1504 - class_loss: 0.4486 - loss: 1.5989 - val_box_loss: 2.9828 - val_class_loss: 3.0315 - val_loss: 6.0143\n",
      "Epoch 14/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 546ms/step - box_loss: 1.1696 - class_loss: 0.4138 - loss: 1.5834 - val_box_loss: 2.3956 - val_class_loss: 3.0572 - val_loss: 5.4528\n",
      "Epoch 15/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 553ms/step - box_loss: 1.1417 - class_loss: 0.4265 - loss: 1.5682 - val_box_loss: 3.2297 - val_class_loss: 2.6827 - val_loss: 5.9124\n",
      "Epoch 16/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 541ms/step - box_loss: 1.0224 - class_loss: 0.3839 - loss: 1.4064 - val_box_loss: 2.8865 - val_class_loss: 2.9791 - val_loss: 5.8656\n",
      "Epoch 17/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 540ms/step - box_loss: 0.9303 - class_loss: 0.3520 - loss: 1.2823 - val_box_loss: 3.3876 - val_class_loss: 2.7362 - val_loss: 6.1238\n",
      "Epoch 18/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 519ms/step - box_loss: 1.0220 - class_loss: 0.3700 - loss: 1.3920 - val_box_loss: 2.2250 - val_class_loss: 3.1379 - val_loss: 5.3628\n",
      "Epoch 19/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 622ms/step - box_loss: 1.1593 - class_loss: 0.4404 - loss: 1.5996 - val_box_loss: 2.3577 - val_class_loss: 2.3992 - val_loss: 4.7568\n",
      "Epoch 20/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 884ms/step - box_loss: 0.9442 - class_loss: 0.3351 - loss: 1.2793 - val_box_loss: 3.3195 - val_class_loss: 1.9372 - val_loss: 5.2567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x76a4e6cd0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=20,\n",
    "    #callbacks=[EvaluateCOCOMetricsCallback(val_ds, \"model.h5\")],\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b95e6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "yolo.save(\"yolo1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c269a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
